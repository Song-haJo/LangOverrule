# Text Dominance Analysis in Multimodal Large Language Models

Re-implementation of **"When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models"** (arXiv:2508.10552, August 2025)

## Overview

This repository provides tools to analyze and mitigate **text dominance** in Multimodal Large Language Models (MLLMs). Text dominance is the phenomenon where MLLMs over-rely on textual inputs while underutilizing information from other modalities (images, video, audio, time-series, graphs).

### Key Features

- **MDI (Modality Dominance Index)**: Quantifies relative attention per token between text and non-text modalities
- **AEI (Attention Efficiency Index)**: Measures attention efficiency relative to token proportion
- **Multi-modality support**: Image, Video, Audio, Time-Series, Graph
- **Token Compression**: [CLS]-based visual token pruning (FasterVLM) to mitigate text dominance

## Installation

```bash
# Clone the repository
git clone https://github.com/your-repo/text-dominance-analysis.git
cd text-dominance-analysis

# Install dependencies
pip install -r requirements.txt
```

### Requirements

- Python 3.9+
- PyTorch 2.0+
- transformers 4.40+
- CUDA (recommended for GPU acceleration)

## ğŸš€ Quick Start with run.sh

### One-Command Execution

```bash
# Run tests and paper comparison
./run.sh test

# Run LLaVA with 10 dummy samples
./run.sh llava 10

# Run Qwen2.5-VL with 100 real MMMU Pro samples
./run.sh qwen 100 real

# Run both models
./run.sh both 50 real
```

## Quick Start (Python API)

### 1. Compute MDI and AEI Metrics

```python
from src.metrics import compute_modality_metrics, MDI, AEI
import torch

# Example: Compute metrics from attention weights
attention_weights = torch.randn(1, 12, 512, 512)  # (batch, heads, seq, seq)
text_mask = torch.zeros(512, dtype=torch.bool)
text_mask[100:200] = True  # Text tokens at positions 100-200
nontext_mask = torch.zeros(512, dtype=torch.bool)
nontext_mask[:100] = True  # Image tokens at positions 0-100

# Compute metrics
metrics = compute_modality_metrics(
    attention_weights,
    text_mask,
    nontext_mask
)
print(f"MDI: {metrics.mdi:.2f}")
print(f"AEI (text): {metrics.aei_text:.2f}")
```

### 2. Analyze LLaVA Model

```python
from src.models import LLaVAWrapper, ModelConfig
from PIL import Image

# Initialize model
config = ModelConfig(
    model_name="llava",
    model_path="llava-hf/llava-1.5-7b-hf",
    device="cuda"
)
wrapper = LLaVAWrapper(config)
wrapper.load_model()

# Analyze image-text sample
result = wrapper.forward_with_attention(
    text="Describe this image in detail.",
    image=Image.open("example.jpg")
)

print(f"MDI (late layers): {result.metrics['late'].mdi:.2f}")
```

### 3. Apply Token Compression

```python
from src.compression import CLSTokenPruner, FasterVLM

# Initialize pruner with 90% reduction
pruner = CLSTokenPruner(reduction_rate=0.90)

# Prune visual tokens
result = pruner.prune(visual_tokens, attention_weights)
print(f"Retained {result.retained_count}/{result.original_count} tokens")
```

### 4. Run Full Analysis

```bash
# Analyze all modalities
python experiments/run_analysis.py --modality all --output-dir results

# Analyze specific modality
python experiments/run_analysis.py --modality image

# Include token compression experiments
python experiments/run_analysis.py --modality image --run-compression
```

## Metrics

### Modality Dominance Index (MDI)

MDI measures relative attention per token between modalities:

```
MDI = (A_T / |T|) / (A_O / |O|)
```

Where:
- `A_T`: Total attention to text tokens
- `A_O`: Total attention to non-text tokens
- `|T|`, `|O|`: Token counts

**Interpretation:**
- MDI > 1: Text dominance
- MDI < 1: Non-text dominance
- MDI â‰ˆ 1: Balanced

### Attention Efficiency Index (AEI)

AEI measures attention efficiency relative to token proportion:

```
AEI_T = P_T / Q_T
```

Where:
- `P_T = A_T / (A_T + A_O)`: Attention proportion
- `Q_T = |T| / (|T| + |O|)`: Token proportion

## Supported Models

| Modality | Model | Status |
|----------|-------|--------|
| Image | LLaVA-1.5-7B/13B | âœ… |
| Image | Qwen2.5-VL | âœ… |
| Image | Kimi-VL | âœ… |
| Video | VideoLLaMA2/3 | âœ… |
| Audio | Qwen2-Audio | âœ… |
| Time-Series | ChatTS | âœ… |
| Graph | GraphGPT | âœ… |

## Key Findings (from Paper)

1. **Text dominance is pervasive**: All tested MLLMs show significant text dominance (MDI >> 1)

2. **Deeper layers amplify dominance**: MDI increases in later transformer layers
   - LLaVA-1.5-7B: Early=1.58, Late=17.37

3. **Token redundancy drives attention dilution**: Non-text modalities have many redundant tokens

4. **Token compression mitigates dominance**: 90% reduction brings MDI from 17.37 to 1.84

## Citation

```bibtex
@article{wu2025when,
  title={When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models},
  author={Wu, Huyu and Tang, Meng and Zheng, Xinhan and Jiang, Haiyun},
  journal={arXiv preprint arXiv:2508.10552},
  year={2025}
}
```

## License

MIT License


## Project êµ¬ì¡°

```
c:\overrule\
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ metrics/           # MDI/AEI ë©”íŠ¸ë¦­ êµ¬í˜„
â”‚   â”‚   â”œâ”€â”€ mdi.py         # Modality Dominance Index
â”‚   â”‚   â”œâ”€â”€ aei.py         # Attention Efficiency Index
â”‚   â”‚   â””â”€â”€ combined.py    # í†µí•© ë©”íŠ¸ë¦­ ê³„ì‚°
â”‚   â”œâ”€â”€ attention/         # ì–´í…ì…˜ ì¶”ì¶œ/ë¶„ì„
â”‚   â”‚   â”œâ”€â”€ extractor.py   # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
â”‚   â”‚   â””â”€â”€ analyzer.py    # í¬ë¡œìŠ¤ëª¨ë‹¬ ì–´í…ì…˜ ë¶„ì„
â”‚   â”œâ”€â”€ models/            # ëª¨ë¸ ë˜í¼
â”‚   â”‚   â”œâ”€â”€ llava.py       # ì´ë¯¸ì§€ (LLaVA)
â”‚   â”‚   â”œâ”€â”€ video_llama.py # ë¹„ë””ì˜¤ (VideoLLaMA)
â”‚   â”‚   â”œâ”€â”€ qwen_audio.py  # ì˜¤ë””ì˜¤ (Qwen-Audio)
â”‚   â”‚   â”œâ”€â”€ timeseries.py  # ì‹œê³„ì—´ (ChatTS)
â”‚   â”‚   â””â”€â”€ graph.py       # ê·¸ë˜í”„ (GraphGPT)
â”‚   â”œâ”€â”€ compression/       # í† í° ì••ì¶•
â”‚   â”‚   â””â”€â”€ token_pruning.py  # [CLS] ê¸°ë°˜ FasterVLM
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ visualization.py  # ì‹œê°í™” (ë…¼ë¬¸ Figure ì¬í˜„)
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ run_analysis.py    # ì‹¤í—˜ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml       # ì„¤ì • íŒŒì¼
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

# í•µì‹¬ êµ¬í˜„ ë‚´ìš©
| êµ¬ì„±ìš”ì†Œ	| íŒŒì¼ 	| ì„¤ëª… |
|-|-|-|
|MDI ë©”íŠ¸ë¦­	| mdi.py	| ìˆ˜ì‹ (1) êµ¬í˜„ |
| AEI ë©”íŠ¸ë¦­	| aei.py	| ìˆ˜ì‹ (2)-(4) êµ¬í˜„ |
| í† í° ì••ì¶•	| token_pruning.py	| ìˆ˜ì‹ (5)-(8) êµ¬í˜„ |
|5ê°œ ëª¨ë‹¬ë¦¬í‹°	|models/	|ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤, ì‹œê³„ì—´, ê·¸ë˜í”„|

ì‹¤ì œ ëª¨ë¸ë¡œ ì‹¤í—˜í•˜ë ¤ë©´ HuggingFaceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤ (ì˜ˆ: llava-hf/llava-1.5-7b-hf).